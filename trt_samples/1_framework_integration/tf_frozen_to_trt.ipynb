{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorRT Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import TensorRT dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import uff\n",
    "    import tensorrt as trt\n",
    "    from tensorrt.parsers import uffparser\n",
    "except ImportError as err:\n",
    "    raise ImportError(\"\"\"ERROR: Failed to import module ({})\n",
    "Please make sure you have the TensorRT Library installed\n",
    "and accessible in your LD_LIBRARY_PATH\"\"\".format(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python dependencies import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from PIL import Image\n",
    "except ImportError as err:\n",
    "    raise ImportError(\"\"\"ERROR: Failed to import module ({})\n",
    "Please make sure you have Pillow installed.\n",
    "For installation instructions, see:\n",
    "http://pillow.readthedocs.io/en/stable/installation.html\"\"\".format(err))\n",
    "\n",
    "try:\n",
    "    import pycuda.driver as cuda\n",
    "    import pycuda.gpuarray as gpuarray\n",
    "    import pycuda.autoinit\n",
    "    import argparse\n",
    "except ImportError as err:\n",
    "    raise ImportError(\"\"\"ERROR: failed to import module ({})\n",
    "Please make sure you have pycuda and the example dependencies installed.\n",
    "https://wiki.tiker.net/PyCuda/Installation/Linux\n",
    "pip(3) install tensorrt[examples]\n",
    "\"\"\".format(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create some Global items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logger creation\n",
    "**Log severity** means log's verbosity level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_LOGGER = trt.infer.ConsoleLogger(trt.infer.LogSeverity.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiler definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Profiler(trt.infer.Profiler):\n",
    "    \"\"\"\n",
    "    Example Implimentation of a Profiler\n",
    "    Is identical to the Profiler class in trt.infer so it is possible\n",
    "    to just use that instead of implementing this if further\n",
    "    functionality is not needed\n",
    "    \"\"\"\n",
    "    def __init__(self, timing_iter):\n",
    "        trt.infer.Profiler.__init__(self)\n",
    "        self.timing_iterations = timing_iter\n",
    "        self.profile = []\n",
    "\n",
    "    def report_layer_time(self, layerName, ms):\n",
    "        record = next((r for r in self.profile if r[0] == layerName), (None, None))\n",
    "        if record == (None, None):\n",
    "            self.profile.append((layerName, ms))\n",
    "        else:\n",
    "            self.profile[self.profile.index(record)] = (record[0], record[1] + ms)\n",
    "\n",
    "    def print_layer_times(self):\n",
    "        totalTime = 0\n",
    "        for i in range(len(self.profile)):\n",
    "            print(\"{:40.40} {:4.3f}ms\".format(self.profile[i][0], self.profile[i][1] / self.timing_iterations))\n",
    "            totalTime += self.profile[i][1]\n",
    "        print(\"Time over all layers: {:4.3f}\".format(totalTime / self.timing_iterations))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operational constants\n",
    "This includes models' information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "G_LOGGER = trt.infer.ConsoleLogger(trt.infer.LogSeverity.INFO)\n",
    "INPUT_LAYERS = [\"input\"]\n",
    "OUTPUT_LAYERS = ['resnet_v2_50/predictions/Reshape_1']\n",
    "\n",
    "DATA_DIR = './frozen_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inception_resnet_v2_frozen.pb\n",
      "inception_v1_frozen.pb\n",
      "inception_v2_frozen.pb\n",
      "inception_v3_frozen.pb\n",
      "inception_v4_frozen.pb\n",
      "resnetV150_frozen.pb\n",
      "resnet_v2_101_frozen.pb\n",
      "resnet_v2_152_frozen.pb\n",
      "resnet_v2_50_frozen.pb\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$DATA_DIR\"\n",
    "ls $1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROZENMODEL = os.path.join(DATA_DIR, \"resnet_v2_50_frozen.pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be sure G_Profiler has created only once\n",
    "try:\n",
    "    TIMING_INTERATIONS\n",
    "except NameError:\n",
    "    TIMING_INTERATIONS = 10000\n",
    "    G_PROFILER = Profiler(TIMING_INTERATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(context, input_img, output_size, batch_size):\n",
    "    #load engine\n",
    "    engine = context.get_engine()\n",
    "    assert(engine.get_nb_bindings() == 2)\n",
    "    #convert input data to Float32\n",
    "    input_img = input_img.astype(np.float32)\n",
    "    #create output array to receive data\n",
    "    output = np.empty(output_size, dtype = np.float32)\n",
    "\n",
    "    #alocate device memory\n",
    "    d_input = cuda.mem_alloc(batch_size * input_img.size * input_img.dtype.itemsize)\n",
    "    d_output = cuda.mem_alloc(batch_size * output.size * output.dtype.itemsize)\n",
    "\n",
    "    bindings = [int(d_input), int(d_output)]\n",
    "\n",
    "    stream = cuda.Stream()\n",
    "\n",
    "    #transfer input data to device\n",
    "    cuda.memcpy_htod_async(d_input, input_img, stream)\n",
    "    #execute model\n",
    "    context.enqueue(batch_size, bindings, stream.handle, None)\n",
    "    #transfer predictions back\n",
    "    cuda.memcpy_dtoh_async(output, d_output, stream)\n",
    "\n",
    "    #synchronize threads\n",
    "    stream.synchronize()\n",
    "\n",
    "    #return predictions\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inference on device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_inference(engine, batch_size):\n",
    "    assert(engine.get_nb_bindings() == 2)\n",
    "\n",
    "    input_index = engine.get_binding_index(INPUT_LAYERS[0])\n",
    "    output_index = engine.get_binding_index(OUTPUT_LAYERS[0])\n",
    "\n",
    "    input_dim = engine.get_binding_dimensions(input_index).to_DimsCHW()\n",
    "    output_dim = engine.get_binding_dimensions(output_index).to_DimsCHW()\n",
    "    \n",
    "    print('dbg:', batch_size, input_dim.C(), input_dim.H(), input_dim.W(), TIMING_INTERATIONS)\n",
    "    insize = batch_size * input_dim.C() * input_dim.H() * input_dim.W() * 4\n",
    "    outsize = batch_size * output_dim.C() * output_dim.H() * output_dim.W() * 4\n",
    "\n",
    "    d_input = cuda.mem_alloc(insize)\n",
    "    d_output = cuda.mem_alloc(outsize)\n",
    "\n",
    "    bindings = [int(d_input), int(d_output)]\n",
    "\n",
    "    context = engine.create_execution_context()\n",
    "    context.set_profiler(G_PROFILER)\n",
    "\n",
    "    cuda.memset_d32(d_input, 0, insize // 4)\n",
    "\n",
    "    for i in range(TIMING_INTERATIONS):\n",
    "        context.execute(batch_size, bindings)\n",
    "\n",
    "    context.destroy()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorrt.infer import Dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    for i in range(len(data)):\n",
    "        data[i] = 1.0 - data[i] / 255.0\n",
    "    return data.reshape(3,224,224)\n",
    "\n",
    "#Lamba to apply argmax to each result after inference to get prediction\n",
    "argmax = lambda res: np.argmax(res.reshape(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using output node resnet_v2_50/predictions/Reshape_1\n",
      "Converting to UFF graph\n",
      "Warning: keep_dims is not supported, ignoring...\n",
      "No. nodes: 475\n"
     ]
    }
   ],
   "source": [
    "uff_model = uff.from_tensorflow_frozen_model(FROZENMODEL, OUTPUT_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building and running GPU inference for GoogleNet, N=32\n",
      "Bindings after deserializing\n",
      "Binding 0 (input): Input\n",
      "Binding 1 (resnet_v2_50/predictions/Reshape_1): Output\n",
      "dbg: 32 3 224 224 10000\n",
      "resnet_v2_50/conv1/BiasAdd               0.579ms\n",
      "(Unnamed Layer* 1)                       0.296ms\n",
      "resnet_v2_50/pool1/MaxPool               0.175ms\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2 0.070ms\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2 0.056ms\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2 0.131ms\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2 0.051ms\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2 0.124ms\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2 0.176ms\n",
      "resnet_v2_50/block1/unit_2/bottleneck_v2 0.131ms\n",
      "resnet_v2_50/block1/unit_2/bottleneck_v2 0.169ms\n",
      "resnet_v2_50/block1/unit_2/bottleneck_v2 0.168ms\n",
      "resnet_v2_50/block1/unit_2/bottleneck_v2 0.104ms\n",
      "resnet_v2_50/block1/unit_2/bottleneck_v2 0.122ms\n",
      "resnet_v2_50/block1/unit_2/bottleneck_v2 0.178ms\n",
      "resnet_v2_50/block1/unit_3/bottleneck_v2 0.045ms\n",
      "resnet_v2_50/block1/unit_3/bottleneck_v2 0.133ms\n",
      "resnet_v2_50/block1/unit_3/bottleneck_v2 0.169ms\n",
      "resnet_v2_50/block1/unit_3/bottleneck_v2 0.167ms\n",
      "resnet_v2_50/block1/unit_3/bottleneck_v2 0.106ms\n",
      "resnet_v2_50/block1/unit_3/bottleneck_v2 0.061ms\n",
      "resnet_v2_50/block1/unit_3/bottleneck_v2 0.058ms\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2 0.039ms\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2 0.049ms\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2 0.046ms\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2 0.129ms\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2 0.057ms\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2 0.120ms\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2 0.115ms\n",
      "resnet_v2_50/block2/unit_2/bottleneck_v2 0.070ms\n",
      "resnet_v2_50/block2/unit_2/bottleneck_v2 0.090ms\n",
      "resnet_v2_50/block2/unit_2/bottleneck_v2 0.085ms\n",
      "resnet_v2_50/block2/unit_2/bottleneck_v2 0.086ms\n",
      "resnet_v2_50/block2/unit_2/bottleneck_v2 0.120ms\n",
      "resnet_v2_50/block2/unit_2/bottleneck_v2 0.114ms\n",
      "resnet_v2_50/block2/unit_3/bottleneck_v2 0.071ms\n",
      "resnet_v2_50/block2/unit_3/bottleneck_v2 0.089ms\n",
      "resnet_v2_50/block2/unit_3/bottleneck_v2 0.085ms\n",
      "resnet_v2_50/block2/unit_3/bottleneck_v2 0.087ms\n",
      "resnet_v2_50/block2/unit_3/bottleneck_v2 0.121ms\n",
      "resnet_v2_50/block2/unit_3/bottleneck_v2 0.114ms\n",
      "resnet_v2_50/block2/unit_4/bottleneck_v2 0.026ms\n",
      "resnet_v2_50/block2/unit_4/bottleneck_v2 0.073ms\n",
      "resnet_v2_50/block2/unit_4/bottleneck_v2 0.090ms\n",
      "resnet_v2_50/block2/unit_4/bottleneck_v2 0.085ms\n",
      "resnet_v2_50/block2/unit_4/bottleneck_v2 0.092ms\n",
      "resnet_v2_50/block2/unit_4/bottleneck_v2 0.058ms\n",
      "resnet_v2_50/block2/unit_4/bottleneck_v2 0.043ms\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2 0.024ms\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2 0.028ms\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2 0.027ms\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2 0.104ms\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2 0.041ms\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2 0.120ms\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2 0.083ms\n",
      "resnet_v2_50/block3/unit_2/bottleneck_v2 0.044ms\n",
      "resnet_v2_50/block3/unit_2/bottleneck_v2 0.050ms\n",
      "resnet_v2_50/block3/unit_2/bottleneck_v2 0.045ms\n",
      "resnet_v2_50/block3/unit_2/bottleneck_v2 0.066ms\n",
      "resnet_v2_50/block3/unit_2/bottleneck_v2 0.120ms\n",
      "resnet_v2_50/block3/unit_2/bottleneck_v2 0.082ms\n",
      "resnet_v2_50/block3/unit_3/bottleneck_v2 0.043ms\n",
      "resnet_v2_50/block3/unit_3/bottleneck_v2 0.050ms\n",
      "resnet_v2_50/block3/unit_3/bottleneck_v2 0.045ms\n",
      "resnet_v2_50/block3/unit_3/bottleneck_v2 0.065ms\n",
      "resnet_v2_50/block3/unit_3/bottleneck_v2 0.120ms\n",
      "resnet_v2_50/block3/unit_3/bottleneck_v2 0.082ms\n",
      "resnet_v2_50/block3/unit_4/bottleneck_v2 0.044ms\n",
      "resnet_v2_50/block3/unit_4/bottleneck_v2 0.049ms\n",
      "resnet_v2_50/block3/unit_4/bottleneck_v2 0.045ms\n",
      "resnet_v2_50/block3/unit_4/bottleneck_v2 0.065ms\n",
      "resnet_v2_50/block3/unit_4/bottleneck_v2 0.120ms\n",
      "resnet_v2_50/block3/unit_4/bottleneck_v2 0.083ms\n",
      "resnet_v2_50/block3/unit_5/bottleneck_v2 0.044ms\n",
      "resnet_v2_50/block3/unit_5/bottleneck_v2 0.049ms\n",
      "resnet_v2_50/block3/unit_5/bottleneck_v2 0.045ms\n",
      "resnet_v2_50/block3/unit_5/bottleneck_v2 0.065ms\n",
      "resnet_v2_50/block3/unit_5/bottleneck_v2 0.120ms\n",
      "resnet_v2_50/block3/unit_5/bottleneck_v2 0.082ms\n",
      "resnet_v2_50/block3/unit_6/bottleneck_v2 0.015ms\n",
      "resnet_v2_50/block3/unit_6/bottleneck_v2 0.047ms\n",
      "resnet_v2_50/block3/unit_6/bottleneck_v2 0.050ms\n",
      "resnet_v2_50/block3/unit_6/bottleneck_v2 0.045ms\n",
      "resnet_v2_50/block3/unit_6/bottleneck_v2 0.065ms\n",
      "resnet_v2_50/block3/unit_6/bottleneck_v2 0.093ms\n",
      "resnet_v2_50/block3/unit_6/bottleneck_v2 0.029ms\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2 0.014ms\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2 0.017ms\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2 0.012ms\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2 0.119ms\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2 0.050ms\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2 0.173ms\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2 0.075ms\n",
      "resnet_v2_50/block4/unit_2/bottleneck_v2 0.025ms\n",
      "resnet_v2_50/block4/unit_2/bottleneck_v2 0.029ms\n",
      "resnet_v2_50/block4/unit_2/bottleneck_v2 0.026ms\n",
      "resnet_v2_50/block4/unit_2/bottleneck_v2 0.088ms\n",
      "resnet_v2_50/block4/unit_2/bottleneck_v2 0.173ms\n",
      "resnet_v2_50/block4/unit_2/bottleneck_v2 0.075ms\n",
      "resnet_v2_50/block4/unit_3/bottleneck_v2 0.025ms\n",
      "resnet_v2_50/block4/unit_3/bottleneck_v2 0.029ms\n",
      "resnet_v2_50/block4/unit_3/bottleneck_v2 0.026ms\n",
      "resnet_v2_50/block4/unit_3/bottleneck_v2 0.088ms\n",
      "resnet_v2_50/block4/unit_3/bottleneck_v2 0.173ms\n",
      "resnet_v2_50/block4/unit_3/bottleneck_v2 0.075ms\n",
      "resnet_v2_50/postnorm/FusedBatchNorm + r 0.025ms\n",
      "resnet_v2_50/postnorm/FusedBatchNorm + r 0.029ms\n",
      "resnet_v2_50/pool5                       0.026ms\n",
      "resnet_v2_50/SpatialSqueeze input reform 0.013ms\n",
      "resnet_v2_50/SpatialSqueeze              0.072ms\n",
      "resnet_v2_50/predictions/Reshape input r 0.010ms\n",
      "resnet_v2_50/predictions/Reshape         0.009ms\n",
      "(Unnamed Layer* 175)                     0.009ms\n",
      "resnet_v2_50/predictions/Softmax         0.012ms\n",
      "resnet_v2_50/predictions/Reshape_1       0.008ms\n",
      "Time over all layers: 9.328\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "path = dir_path = \"./\"\n",
    "\n",
    "print(\"Building and running GPU inference for GoogleNet, N=%d\" % (BATCH_SIZE))\n",
    "\n",
    "#Convert caffe model to TensorRT engine\n",
    "# engine = trt.utils.caffe_to_trt_engine(G_LOGGER,\n",
    "#     MODEL_PROTOTXT,\n",
    "#     CAFFEMODEL,\n",
    "#     10,\n",
    "#     16 << 20,\n",
    "#     OUTPUT_LAYERS,\n",
    "#     trt.infer.DataType.FLOAT)\n",
    "\n",
    "# uff_model = uff.from_tensorflow_frozen_model(FROZENMODEL, OUTPUT_LAYERS)\n",
    "uff_parser = uffparser.create_uff_parser()\n",
    "uff_parser.register_input(INPUT_LAYERS[0], (3, 224, 224), 0)\n",
    "uff_parser.register_output(OUTPUT_LAYERS[0])\n",
    "\n",
    "engine = trt.utils.uff_to_trt_engine(\n",
    "    logger=G_LOGGER,\n",
    "    stream=uff_model,\n",
    "    parser=uff_parser,\n",
    "    max_batch_size=BATCH_SIZE,\n",
    "    max_workspace_size=1 << 20,\n",
    "    datatype=trt.infer.DataType.HALF,\n",
    "    plugin_factory=None,\n",
    "    calibrator=None\n",
    ")\n",
    "\n",
    "runtime = trt.infer.create_infer_runtime(G_LOGGER)\n",
    "\n",
    "print(\"Bindings after deserializing\")\n",
    "for bi in range(engine.get_nb_bindings()):\n",
    "    if engine.binding_is_input(bi) == True:\n",
    "        print(\"Binding \" + str(bi) + \" (\" + engine.get_binding_name(bi) + \"): Input\")\n",
    "    else:\n",
    "        print(\"Binding \" + str(bi) + \" (\" + engine.get_binding_name(bi) + \"): Output\")\n",
    "\n",
    "time_inference(engine, BATCH_SIZE)\n",
    "\n",
    "engine.destroy()\n",
    "runtime.destroy()\n",
    "\n",
    "G_PROFILER.print_layer_times()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
