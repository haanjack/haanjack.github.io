{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorRT Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import tensorrt as trt\n",
    "    from tensorrt import parsers\n",
    "except ImportError as err:\n",
    "    raise ImportError(\"\"\"ERROR: Flailed to import module ({})\n",
    "Please make sure you have the TensorRT Library installed\n",
    "and accessible in your LD_LIBRARY_PATH\"\"\".format(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python dependencies import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from PIL import Image\n",
    "except ImportError as err:\n",
    "    raise ImportError(\"\"\"ERROR: Failed to import module ({})\n",
    "Please make sure you have Pillow installed.\n",
    "For installation instructions, see:\n",
    "http://pillow.readthedocs.io/en/stable/installation.html\"\"\".format(err))\n",
    "\n",
    "try:\n",
    "    import pycuda.driver as cuda\n",
    "    import pycuda.gpuarray as gpuarray\n",
    "    import pycuda.autoinit\n",
    "    import argparse\n",
    "except ImportError as err:\n",
    "    raise ImportError(\"\"\"ERROR: failed to import module ({})\n",
    "Please make sure you have pycuda and the example dependencies installed.\n",
    "https://wiki.tiker.net/PyCuda/Installation/Linux\n",
    "pip(3) install tensorrt[examples]\n",
    "\"\"\".format(err))\n",
    "    \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\" #selects a specific device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create some Global items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logger creation\n",
    "**Log severity** means log's verbosity level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_LOGGER = trt.infer.ConsoleLogger(trt.infer.LogSeverity.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiler definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Profiler(trt.infer.Profiler):\n",
    "    \"\"\"\n",
    "    Example Implimentation of a Profiler\n",
    "    Is identical to the Profiler class in trt.infer so it is possible\n",
    "    to just use that instead of implementing this if further\n",
    "    functionality is not needed\n",
    "    \"\"\"\n",
    "    def __init__(self, timing_iter):\n",
    "        trt.infer.Profiler.__init__(self)\n",
    "        self.timing_iterations = timing_iter\n",
    "        self.profile = []\n",
    "\n",
    "    def report_layer_time(self, layerName, ms):\n",
    "        record = next((r for r in self.profile if r[0] == layerName), (None, None))\n",
    "        if record == (None, None):\n",
    "            self.profile.append((layerName, ms))\n",
    "        else:\n",
    "            self.profile[self.profile.index(record)] = (record[0], record[1] + ms)\n",
    "\n",
    "    def print_layer_times(self):\n",
    "        totalTime = 0\n",
    "        for i in range(len(self.profile)):\n",
    "            print(\"{:40.40} {:4.3f}ms\".format(self.profile[i][0], self.profile[i][1] / self.timing_iterations))\n",
    "            totalTime += self.profile[i][1]\n",
    "        print(\"Time over all layers: {:4.3f}\".format(totalTime / self.timing_iterations))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operational constants\n",
    "This includes models' information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "G_LOGGER = trt.infer.ConsoleLogger(trt.infer.LogSeverity.INFO)\n",
    "INPUT_LAYERS = [\"data\"]\n",
    "OUTPUT_LAYERS = ['prob']\n",
    "\n",
    "DATA_DIR = '/workspace/tensorrt/python/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be sure G_Profiler has created only once\n",
    "try:\n",
    "    TIMING_INTERATIONS\n",
    "except NameError:\n",
    "    TIMING_INTERATIONS = 1000\n",
    "    G_PROFILER = Profiler(TIMING_INTERATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PROTOTXT = DATA_DIR + \"/googlenet/googlenet.prototxt\"\n",
    "CAFFEMODEL = DATA_DIR + \"/googlenet/googlenet.caffemodel\"\n",
    "DATA =  DATA_DIR + '/googlenet/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(context, input_img, output_size, batch_size):\n",
    "    #load engine\n",
    "    engine = context.get_engine()\n",
    "    assert(engine.get_nb_bindings() == 2)\n",
    "    #convert input data to Float32\n",
    "    input_img = input_img.astype(np.float32)\n",
    "    #create output array to receive data\n",
    "    output = np.empty(output_size, dtype = np.float32)\n",
    "\n",
    "    #alocate device memory\n",
    "    d_input = cuda.mem_alloc(batch_size * input_img.size * input_img.dtype.itemsize)\n",
    "    d_output = cuda.mem_alloc(batch_size * output.size * output.dtype.itemsize)\n",
    "\n",
    "    bindings = [int(d_input), int(d_output)]\n",
    "\n",
    "    stream = cuda.Stream()\n",
    "\n",
    "    #transfer input data to device\n",
    "    cuda.memcpy_htod_async(d_input, input_img, stream)\n",
    "    #execute model\n",
    "    context.enqueue(batch_size, bindings, stream.handle, None)\n",
    "    #transfer predictions back\n",
    "    cuda.memcpy_dtoh_async(output, d_output, stream)\n",
    "\n",
    "    #synchronize threads\n",
    "    stream.synchronize()\n",
    "\n",
    "    #return predictions\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inference on device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_inference(engine, batch_size):\n",
    "    assert(engine.get_nb_bindings() == 2)\n",
    "\n",
    "    input_index = engine.get_binding_index(INPUT_LAYERS[0])\n",
    "    output_index = engine.get_binding_index(OUTPUT_LAYERS[0])\n",
    "\n",
    "    input_dim = engine.get_binding_dimensions(input_index).to_DimsCHW()\n",
    "    output_dim = engine.get_binding_dimensions(output_index).to_DimsCHW()\n",
    "\n",
    "    insize = batch_size * input_dim.C() * input_dim.H() * input_dim.W() * 4\n",
    "    outsize = batch_size * output_dim.C() * output_dim.H() * output_dim.W() * 4\n",
    "\n",
    "    d_input = cuda.mem_alloc(insize)\n",
    "    d_output = cuda.mem_alloc(outsize)\n",
    "\n",
    "    bindings = [int(d_input), int(d_output)]\n",
    "\n",
    "    context = engine.create_execution_context()\n",
    "    context.set_profiler(G_PROFILER)\n",
    "\n",
    "    cuda.memset_d32(d_input, 0, insize // 4)\n",
    "\n",
    "    for i in range(TIMING_INTERATIONS):\n",
    "        context.execute(batch_size, bindings)\n",
    "\n",
    "    context.destroy()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building and running GPU inference for GoogleNet, N=8\n"
     ]
    }
   ],
   "source": [
    "path = dir_path = \"./\"\n",
    "\n",
    "print(\"Building and running GPU inference for GoogleNet, N=%d\" % (BATCH_SIZE))\n",
    "#Convert caffe model to TensorRT engine\n",
    "engine = trt.utils.caffe_to_trt_engine(G_LOGGER,\n",
    "    MODEL_PROTOTXT,\n",
    "    CAFFEMODEL,\n",
    "    10,\n",
    "    16 << 20,\n",
    "    OUTPUT_LAYERS,\n",
    "    trt.infer.DataType.FLOAT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bindings after deserializing\n",
      "Binding 0 (data): Input\n",
      "Binding 1 (prob): Output\n",
      "conv1/7x7_s2 + conv1/relu_7x7            0.171ms\n",
      "pool1/3x3_s2                             0.051ms\n",
      "pool1/norm1                              0.037ms\n",
      "conv2/3x3_reduce + conv2/relu_3x3_reduce 0.040ms\n",
      "conv2/3x3 + conv2/relu_3x3               0.259ms\n",
      "conv2/norm2                              0.101ms\n",
      "pool2/3x3_s2                             0.041ms\n",
      "inception_3a/1x1 + inception_3a/relu_1x1 0.054ms\n",
      "inception_3a/3x3 + inception_3a/relu_3x3 0.093ms\n",
      "inception_3a/5x5 + inception_3a/relu_5x5 0.060ms\n",
      "inception_3a/pool                        0.021ms\n",
      "inception_3a/pool_proj + inception_3a/re 0.040ms\n",
      "inception_3a/1x1 copy                    0.012ms\n",
      "inception_3b/1x1 + inception_3b/relu_1x1 0.109ms\n",
      "inception_3b/3x3 + inception_3b/relu_3x3 0.148ms\n",
      "inception_3b/5x5 + inception_3b/relu_5x5 0.157ms\n",
      "inception_3b/pool                        0.026ms\n",
      "inception_3b/pool_proj + inception_3b/re 0.048ms\n",
      "inception_3b/1x1 copy                    0.016ms\n",
      "pool3/3x3_s2                             0.030ms\n",
      "inception_4a/1x1 + inception_4a/relu_1x1 0.067ms\n",
      "inception_4a/3x3 + inception_4a/relu_3x3 0.055ms\n",
      "inception_4a/5x5 + inception_4a/relu_5x5 0.060ms\n",
      "inception_4a/pool                        0.016ms\n",
      "inception_4a/pool_proj + inception_4a/re 0.069ms\n",
      "inception_4a/1x1 copy                    0.010ms\n",
      "inception_4b/1x1 + inception_4b/relu_1x1 0.070ms\n",
      "inception_4b/3x3 + inception_4b/relu_3x3 0.061ms\n",
      "inception_4b/5x5 + inception_4b/relu_5x5 0.081ms\n",
      "inception_4b/pool                        0.017ms\n",
      "inception_4b/pool_proj + inception_4b/re 0.072ms\n",
      "inception_4b/1x1 copy                    0.010ms\n",
      "inception_4c/1x1 + inception_4c/relu_1x1 0.070ms\n",
      "inception_4c/3x3 + inception_4c/relu_3x3 0.067ms\n",
      "inception_4c/5x5 + inception_4c/relu_5x5 0.081ms\n",
      "inception_4c/pool                        0.016ms\n",
      "inception_4c/pool_proj + inception_4c/re 0.072ms\n",
      "inception_4c/1x1 copy                    0.010ms\n",
      "inception_4d/1x1 + inception_4d/relu_1x1 0.070ms\n",
      "inception_4d/3x3 + inception_4d/relu_3x3 0.073ms\n",
      "inception_4d/5x5 + inception_4d/relu_5x5 0.088ms\n",
      "inception_4d/pool                        0.017ms\n",
      "inception_4d/pool_proj + inception_4d/re 0.069ms\n",
      "inception_4d/1x1 copy                    0.009ms\n",
      "inception_4e/1x1 + inception_4e/relu_1x1 0.105ms\n",
      "inception_4e/3x3 + inception_4e/relu_3x3 0.083ms\n",
      "inception_4e/5x5 + inception_4e/relu_5x5 0.101ms\n",
      "inception_4e/pool                        0.017ms\n",
      "inception_4e/pool_proj + inception_4e/re 0.072ms\n",
      "inception_4e/1x1 copy                    0.012ms\n",
      "pool4/3x3_s2                             0.016ms\n",
      "inception_5a/1x1 + inception_5a/relu_1x1 0.101ms\n",
      "inception_5a/3x3 + inception_5a/relu_3x3 0.051ms\n",
      "inception_5a/5x5 + inception_5a/relu_5x5 0.101ms\n",
      "inception_5a/pool                        0.012ms\n",
      "inception_5a/pool_proj + inception_5a/re 0.100ms\n",
      "inception_5a/1x1 copy                    0.008ms\n",
      "inception_5b/1x1 + inception_5b/relu_1x1 0.096ms\n",
      "inception_5b/3x3 + inception_5b/relu_3x3 0.091ms\n",
      "inception_5b/5x5 + inception_5b/relu_5x5 0.125ms\n",
      "inception_5b/pool                        0.012ms\n",
      "inception_5b/pool_proj + inception_5b/re 0.100ms\n",
      "inception_5b/1x1 copy                    0.009ms\n",
      "pool5/7x7_s1                             0.014ms\n",
      "loss3/classifier                         0.033ms\n",
      "prob                                     0.010ms\n",
      "Time over all layers: 4.012\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "runtime = trt.infer.create_infer_runtime(G_LOGGER)\n",
    "\n",
    "print(\"Bindings after deserializing\")\n",
    "for bi in range(engine.get_nb_bindings()):\n",
    "    if engine.binding_is_input(bi) == True:\n",
    "        print(\"Binding \" + str(bi) + \" (\" + engine.get_binding_name(bi) + \"): Input\")\n",
    "    else:\n",
    "        print(\"Binding \" + str(bi) + \" (\" + engine.get_binding_name(bi) + \"): Output\")\n",
    "\n",
    "time_inference(engine, BATCH_SIZE)\n",
    "\n",
    "G_PROFILER.print_layer_times()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.destroy()\n",
    "runtime.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
