{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow-TensorRT Sample\n",
    "This notebook shows simple process of model optimization from TensorFlow to TensorRT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependent packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import data_flow_ops\n",
    "import tensorflow.contrib.tensorrt as trt\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.python.platform import gfile\n",
    "from tensorflow.python.client import timeline\n",
    "import argparse, sys, itertools,datetime\n",
    "import json\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #selects a specific device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorRT integration options\n",
    "**Output Layer**'s name can be obtained from the model freezing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # models\n",
    "    \"frozen_model_file\": \"./frozen_models/resnetV150_frozen.pb\",\n",
    "    \"output_layer\": \"resnet_v1_50/predictions/Reshape_1\",\n",
    "    \n",
    "    # Parameters\n",
    "    \"FP32\": True,\n",
    "    \"FP16\": True,\n",
    "    \"INT8\": True,\n",
    "    \"native\": True,\n",
    "    \"num_loops\": 20,\n",
    "    \"topN\": 10,\n",
    "    \"batch_size\": 128,\n",
    "    \"dump_diff\": True,\n",
    "    \"with_timeline\": True,\n",
    "    \"workspace_size\": 1<<10,\n",
    "    \"update_graphdef\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tensor_from_image_file(file_name, input_height=224, input_width=224,\n",
    "                                input_mean=0, input_std=255):\n",
    "  \"\"\" Read a jpg image file and return a tensor \"\"\"\n",
    "  input_name = \"file_reader\"\n",
    "  output_name = \"normalized\"\n",
    "  file_reader = tf.read_file(file_name, input_name)\n",
    "  image_reader = tf.image.decode_png(file_reader, channels = 3,\n",
    "                                       name='jpg_reader')\n",
    "  float_caster = tf.cast(image_reader, tf.float32)\n",
    "  dims_expander = tf.expand_dims(float_caster, 0);\n",
    "  resized = tf.image.resize_bilinear(dims_expander, [input_height, input_width])\n",
    "  normalized = tf.divide(tf.subtract(resized, [input_mean]), [input_std])\n",
    "  sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.50)))\n",
    "  result = sess.run([normalized,tf.transpose(normalized,perm=(0,3,1,2))])\n",
    "  del sess\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimpleGraphDef():\n",
    "  \"\"\"Create a simple graph and return its graph_def\"\"\"\n",
    "  if gfile.Exists(\"origgraph\"):\n",
    "    gfile.DeleteRecursively(\"origgraph\")\n",
    "  g = tf.Graph()\n",
    "  with g.as_default():\n",
    "    A = tf.placeholder(dtype=tf.float32, shape=(None, 224, 224, 3), name=\"input\")\n",
    "    e = tf.constant(\n",
    "        [[[[1., 0.5, 4., 6., 0.5, 1.], [1., 0.5, 1., 1., 0.5, 1.],[1.,1.,1.,1.,1.,1.]]]],\n",
    "        name=\"weights\",\n",
    "        dtype=tf.float32)\n",
    "    conv = tf.nn.conv2d(\n",
    "        input=A, filter=e, strides=[1, 1, 1, 1],dilations=[1,1,1,1], padding=\"SAME\", name=\"conv\")\n",
    "    b = tf.constant([4., 1.5, 2., 3., 5., 7.], name=\"bias\", dtype=tf.float32)\n",
    "    t = tf.nn.bias_add(conv, b, name=\"biasAdd\")\n",
    "    relu = tf.nn.relu(t, \"relu\")\n",
    "    idty = tf.identity(relu, \"ID\")\n",
    "    v = tf.nn.max_pool(\n",
    "        idty, [1, 2, 2, 1], [1, 2, 2, 1], \"VALID\", name=\"max_pool\")\n",
    "    out = tf.squeeze(v, name=config[\"output_layer\"])\n",
    "    writer = tf.summary.FileWriter(\"origgraph\", g)\n",
    "    writer.close()\n",
    "    \n",
    "  return g.as_graph_def()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResnet50():\n",
    "  with gfile.FastGFile(config[\"frozen_model_file\"], 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "  return graph_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateGraphDef(fileName):\n",
    "  with gfile.FastGFile(fileName,'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "  tf.reset_default_graph()\n",
    "  g=tf.Graph()\n",
    "  with g.as_default():\n",
    "    tf.import_graph_def(graph_def,name=\"\")\n",
    "    with gfile.FastGFile(fileName,'wb') as f:\n",
    "      f.write(g.as_graph_def().SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printStats(graphName,timings,batch_size):\n",
    "  if timings is None:\n",
    "    return\n",
    "  times=np.array(timings)\n",
    "  speeds=batch_size / times\n",
    "  avgTime=np.mean(timings)\n",
    "  avgSpeed=batch_size/avgTime\n",
    "  stdTime=np.std(timings)\n",
    "  stdSpeed=np.std(speeds)\n",
    "  print(\"images/s : %.1f +/- %.1f, s/batch: %.5f +/- %.5f\"%(avgSpeed,stdSpeed,avgTime,stdTime))\n",
    "  print(\"RES, %s, %s, %.2f, %.2f, %.5f, %.5f\"%(graphName,batch_size,avgSpeed,stdSpeed,avgTime,stdTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFP32(batch_size=128,workspace_size=1<<30):\n",
    "  trt_graph = trt.create_inference_graph(getResnet50(), [ config[\"output_layer\"] ],\n",
    "                                         max_batch_size=batch_size,\n",
    "                                         max_workspace_size_bytes=workspace_size,\n",
    "                                         precision_mode=\"FP32\")  # Get optimized graph\n",
    "  with gfile.FastGFile(\"resnetV150_TRTFP32.pb\",'wb') as f:\n",
    "    f.write(trt_graph.SerializeToString())\n",
    "  return trt_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFP16(batch_size=128,workspace_size=1<<30):\n",
    "  trt_graph = trt.create_inference_graph(getResnet50(), [ config[\"output_layer\"] ],\n",
    "                                         max_batch_size=batch_size,\n",
    "                                         max_workspace_size_bytes=workspace_size,\n",
    "                                         precision_mode=\"FP16\")  # Get optimized graph\n",
    "  with gfile.FastGFile(\"resnetV150_TRTFP16.pb\",'wb') as f:\n",
    "    f.write(trt_graph.SerializeToString())\n",
    "  return trt_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making INT8 calibration using create_inference_graph. The output is a frozen ready for calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getINT8CalibGraph(batch_size=128,workspace_size=1<<30):\n",
    "  trt_graph = trt.create_inference_graph(getResnet50(), [ config[\"output_layer\"] ],\n",
    "                                         max_batch_size=batch_size,\n",
    "                                         max_workspace_size_bytes=workspace_size,\n",
    "                                         precision_mode=\"INT8\")  # calibration\n",
    "  with gfile.FastGFile(\"resnetV150_TRTINT8Calib.pb\",'wb') as f:\n",
    "    f.write(trt_graph.SerializeToString())\n",
    "  return trt_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, we do calibration with trt calibration graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getINT8InferenceGraph(calibGraph):\n",
    "  trt_graph=trt.calib_graph_to_infer_graph(calibGraph)\n",
    "  with gfile.FastGFile(\"resnetV150_TRTINT8.pb\",'wb') as f:\n",
    "    f.write(trt_graph.SerializeToString())\n",
    "  return trt_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeGraph(gdef,batch_size=128,num_loops=100,dummy_input=None,timelineName=None):\n",
    "  tf.logging.info(\"Starting execution\")\n",
    "  gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.50)\n",
    "  tf.reset_default_graph()\n",
    "  g = tf.Graph()\n",
    "  if dummy_input is None:\n",
    "    dummy_input = np.random.random_sample((batch_size,224,224,3))\n",
    "  outlist=[]\n",
    "  with g.as_default():\n",
    "    inc=tf.constant(dummy_input, dtype=tf.float32)\n",
    "    dataset=tf.data.Dataset.from_tensors(inc)\n",
    "    dataset=dataset.repeat()\n",
    "    iterator=dataset.make_one_shot_iterator()\n",
    "    next_element=iterator.get_next()\n",
    "    out = tf.import_graph_def(\n",
    "      graph_def=gdef,\n",
    "      input_map={\"input\":next_element},\n",
    "      return_elements=[ config[\"output_layer\"] ]\n",
    "    )\n",
    "    out = out[0].outputs[0]\n",
    "    outlist.append(out)\n",
    "    \n",
    "  timings=[]\n",
    "\n",
    "  with tf.Session(graph=g,config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "    run_metadata = tf.RunMetadata()\n",
    "    tf.logging.info(\"Starting Warmup cycle\")\n",
    "    def mergeTraceStr(mdarr):\n",
    "      tl=timeline.Timeline(mdarr[0][0].step_stats)\n",
    "      ctf=tl.generate_chrome_trace_format()\n",
    "      Gtf=json.loads(ctf)\n",
    "      deltat=mdarr[0][1][1]\n",
    "      for md in mdarr[1:]:\n",
    "        tl=timeline.Timeline(md[0].step_stats)\n",
    "        ctf=tl.generate_chrome_trace_format()\n",
    "        tmp=json.loads(ctf)\n",
    "        deltat=0\n",
    "        Gtf[\"traceEvents\"].extend(tmp[\"traceEvents\"])\n",
    "        deltat=md[1][1]\n",
    "        \n",
    "      return json.dumps(Gtf,indent=2)\n",
    "    rmArr=[[tf.RunMetadata(),0] for x in range(20)]\n",
    "    if timelineName:\n",
    "      if gfile.Exists(timelineName):\n",
    "        gfile.Remove(timelineName)\n",
    "      ttot=int(0)\n",
    "      tend=time.time()\n",
    "      for i in range(20):\n",
    "        tstart=time.time()\n",
    "        valt = sess.run(outlist,options=run_options,run_metadata=rmArr[i][0])\n",
    "        tend=time.time()\n",
    "        rmArr[i][1]=(int(tstart*1.e6),int(tend*1.e6))\n",
    "      with gfile.FastGFile(timelineName,\"a\") as tlf:\n",
    "        tlf.write(mergeTraceStr(rmArr))\n",
    "    else:\n",
    "      for i in range(20):\n",
    "        valt = sess.run(outlist)\n",
    "    tf.logging.info(\"Warmup done. Starting real timing\")\n",
    "    num_iters=50\n",
    "    for i in range(num_loops):\n",
    "      tstart=time.time()\n",
    "      for k in range(num_iters):\n",
    "        val = sess.run(outlist)\n",
    "      timings.append((time.time()-tstart)/float(num_iters))\n",
    "      print(\"iter \",i,\" \",timings[-1])\n",
    "    comp=sess.run(tf.reduce_all(tf.equal(val[0],valt[0])))\n",
    "    print(\"Comparison=\",comp)\n",
    "    sess.close()\n",
    "    tf.logging.info(\"Timing loop done!\")\n",
    "    return timings,comp,val[0],None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(nat,trt,topN=5):\n",
    "  ind=np.argsort(nat)[:,-topN:]\n",
    "  tind=np.argsort(trt)[:,-topN:]\n",
    "  return np.array_equal(ind,tind),howClose(nat,trt,topN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topX(arr,X):\n",
    "  ind=np.argsort(arr)[:,-X:][:,::-1]\n",
    "  return arr[np.arange(np.shape(arr)[0])[:,np.newaxis],ind],ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def howClose(arr1,arr2,X):\n",
    "  val1,ind1=topX(arr1,X)\n",
    "  val2,ind2=topX(arr2,X)\n",
    "  ssum=0.\n",
    "  for i in range(X):\n",
    "    in1=ind1[0]\n",
    "    in2=ind2[0]\n",
    "    if(in1[i]==in2[i]):\n",
    "      ssum+=1\n",
    "    else:\n",
    "      pos=np.where(in2==in1[i])\n",
    "      pos=pos[0]\n",
    "      if pos.shape[0]:\n",
    "        if np.abs(pos[0]-i)<2:\n",
    "          ssum+=0.5\n",
    "  return ssum/X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabels(labels,ids):\n",
    "  return [labels[str(x+1)] for x in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at 2018-07-30 04:11:28.817866\n",
      "INFO:tensorflow:Starting execution\n",
      "INFO:tensorflow:Starting Warmup cycle\n",
      "INFO:tensorflow:Warmup done. Starting real timing\n",
      "iter  0   0.10343759059906006\n",
      "iter  1   0.10354409217834473\n",
      "iter  2   0.10343996047973633\n",
      "iter  3   0.10382778644561767\n",
      "iter  4   0.1024544334411621\n",
      "iter  5   0.10323407173156739\n",
      "iter  6   0.10393102169036865\n",
      "iter  7   0.10415175437927246\n",
      "iter  8   0.10453024864196778\n",
      "iter  9   0.103573637008667\n",
      "iter  10   0.10539804458618164\n",
      "iter  11   0.10416852951049804\n",
      "iter  12   0.1039428472518921\n",
      "iter  13   0.10392391681671143\n",
      "iter  14   0.1047536039352417\n",
      "iter  15   0.10394089698791503\n",
      "iter  16   0.10423333168029786\n",
      "iter  17   0.1036576795578003\n",
      "iter  18   0.10333051681518554\n",
      "iter  19   0.10330786228179932\n",
      "Comparison= True\n",
      "INFO:tensorflow:Timing loop done!\n",
      "images/s : 1232.7 +/- 7.2, s/batch: 0.10384 +/- 0.00061\n",
      "RES, Native, 128, 1232.68, 7.24, 0.10384, 0.00061\n",
      "INFO:tensorflow:Starting execution\n",
      "INFO:tensorflow:Starting Warmup cycle\n",
      "INFO:tensorflow:Warmup done. Starting real timing\n",
      "iter  0   0.10434460163116455\n",
      "iter  1   0.10463499069213868\n",
      "iter  2   0.10486001014709473\n",
      "iter  3   0.10384658336639405\n",
      "iter  4   0.10461666584014892\n",
      "iter  5   0.10392025470733643\n",
      "iter  6   0.1042506217956543\n",
      "iter  7   0.10407578945159912\n",
      "iter  8   0.10525691032409668\n",
      "iter  9   0.10420670509338378\n",
      "iter  10   0.10448449611663818\n",
      "iter  11   0.10417623043060303\n",
      "iter  12   0.10366077423095703\n",
      "iter  13   0.10489983081817628\n",
      "iter  14   0.10518900394439697\n",
      "iter  15   0.10422153949737549\n",
      "iter  16   0.10452737331390381\n",
      "iter  17   0.10387077808380127\n",
      "iter  18   0.10538424491882324\n",
      "iter  19   0.1042972183227539\n",
      "Comparison= True\n",
      "INFO:tensorflow:Timing loop done!\n",
      "images/s : 1225.6 +/- 5.6, s/batch: 0.10444 +/- 0.00047\n",
      "RES, TRT-FP32, 128, 1225.63, 5.56, 0.10444, 0.00047\n",
      "INFO:tensorflow:Starting execution\n",
      "INFO:tensorflow:Starting Warmup cycle\n",
      "INFO:tensorflow:Warmup done. Starting real timing\n",
      "iter  0   0.10440097332000732\n",
      "iter  1   0.10445652961730957\n",
      "iter  2   0.10544662952423095\n",
      "iter  3   0.1050718069076538\n",
      "iter  4   0.10385618209838868\n",
      "iter  5   0.10393055915832519\n",
      "iter  6   0.10407494068145752\n",
      "iter  7   0.10434704303741454\n",
      "iter  8   0.10486339092254639\n",
      "iter  9   0.1047061014175415\n",
      "iter  10   0.10425742626190186\n",
      "iter  11   0.10458285331726075\n",
      "iter  12   0.10450096130371093\n",
      "iter  13   0.10352264881134034\n",
      "iter  14   0.1044005823135376\n",
      "iter  15   0.10498286247253417\n",
      "iter  16   0.10523305416107177\n",
      "iter  17   0.10533883571624755\n",
      "iter  18   0.1040876293182373\n",
      "iter  19   0.10377810001373292\n",
      "Comparison= True\n",
      "INFO:tensorflow:Timing loop done!\n",
      "images/s : 1225.0 +/- 6.2, s/batch: 0.10449 +/- 0.00053\n",
      "RES, TRT-FP16, 128, 1224.97, 6.17, 0.10449, 0.00053\n",
      "Running Calibration\n",
      "INFO:tensorflow:Starting execution\n",
      "INFO:tensorflow:Starting Warmup cycle\n"
     ]
    }
   ],
   "source": [
    "valnative=None\n",
    "valfp32=None\n",
    "valfp16=None\n",
    "valint8=None\n",
    "res=[None,None,None,None]\n",
    "\n",
    "print(\"Starting at\",datetime.datetime.now())\n",
    "\n",
    "if config[\"update_graphdef\"]:\n",
    "    updateGraphDef(config[\"frozen_model_file\"])\n",
    "dummy_input = np.random.random_sample((config[\"batch_size\"],224,224,3))\n",
    "with open(\"labellist.json\",\"r\") as lf:\n",
    "    labels=json.load(lf)\n",
    "imageName=\"grace_hopper.jpg\"\n",
    "t = read_tensor_from_image_file(imageName,\n",
    "                              input_height=224,\n",
    "                              input_width=224,\n",
    "                              input_mean=0,\n",
    "                              input_std=1.0)\n",
    "tshape=list(t[0].shape)\n",
    "tshape[0]=config[\"batch_size\"]\n",
    "tnhwcbatch=np.tile(t[0],(config[\"batch_size\"],1,1,1))\n",
    "dummy_input=tnhwcbatch\n",
    "wsize=config[\"workspace_size\"]<<20\n",
    "timelineName=None\n",
    "if config[\"native\"]:\n",
    "    if config[\"with_timeline\"]: timelineName=\"NativeTimeline.json\"\n",
    "    timings,comp,valnative,mdstats=timeGraph(getResnet50(),config[\"batch_size\"],\n",
    "                                 config[\"num_loops\"],dummy_input,timelineName)\n",
    "    printStats(\"Native\",timings,config[\"batch_size\"])\n",
    "    printStats(\"NativeRS\",mdstats,config[\"batch_size\"])\n",
    "    \n",
    "if config[\"FP32\"]:\n",
    "    if config[\"with_timeline\"]: timelineName=\"FP32Timeline.json\"\n",
    "    timings,comp,valfp32,mdstats=timeGraph(getFP32(config[\"batch_size\"],wsize),config[\"batch_size\"],config[\"num_loops\"],\n",
    "                               dummy_input,timelineName)\n",
    "    printStats(\"TRT-FP32\",timings,config[\"batch_size\"])\n",
    "    printStats(\"TRT-FP32RS\",mdstats,config[\"batch_size\"])\n",
    "    \n",
    "if config[\"FP16\"]:\n",
    "    k=0\n",
    "    if config[\"with_timeline\"]: timelineName=\"FP16Timeline.json\"\n",
    "    timings,comp,valfp16,mdstats=timeGraph(getFP16(config[\"batch_size\"],wsize),config[\"batch_size\"],\n",
    "                                   config[\"num_loops\"],dummy_input,timelineName)\n",
    "    printStats(\"TRT-FP16\",timings,config[\"batch_size\"])\n",
    "    printStats(\"TRT-FP16RS\",mdstats,config[\"batch_size\"])\n",
    "    \n",
    "if config[\"INT8\"]:\n",
    "    calibGraph=getINT8CalibGraph(config[\"batch_size\"],wsize)\n",
    "    print(\"Running Calibration\")\n",
    "    timings,comp,_,mdstats=timeGraph(calibGraph,config[\"batch_size\"],1,dummy_input)\n",
    "    print(\"Creating inference graph\")\n",
    "    int8Graph=getINT8InferenceGraph(calibGraph)\n",
    "    del calibGraph\n",
    "    if config[\"with_timeline\"]: timelineName=\"INT8Timeline.json\"\n",
    "    timings,comp,valint8,mdstats=timeGraph(int8Graph,config[\"batch_size\"],\n",
    "                                   config[\"num_loops\"],dummy_input,timelineName)\n",
    "    printStats(\"TRT-INT8\",timings,config[\"batch_size\"])\n",
    "    printStats(\"TRT-INT8RS\",mdstats,config[\"batch_size\"])\n",
    "vals=[valnative,valfp32,valfp16,valint8]\n",
    "enabled=[(config[\"native\"],\"native\",valnative),\n",
    "       (config[\"FP32\"],\"FP32\",valfp32),\n",
    "       (config[\"FP16\"],\"FP16\",valfp16),\n",
    "       (config[\"INT8\"],\"INT8\",valint8)]\n",
    "print(\"Done timing\",datetime.datetime.now())\n",
    "for i in enabled:\n",
    "    if i[0]:\n",
    "        print(i[1],getLabels(labels,topX(i[2],config[\"topN\"])[1][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
