---
layout: post
title:  "DCGM을 활용한 GPU서버 모니터링"
description: "Data Center에서 GPU 모니터링을 위한 DCGM 라이브러리 소개 및 활용"
date:   2018-06-01
published: false
categories: GPU
tags:
 - nvidia
 - GPU
 - monitor
---

酷达科技是深度学习软硬件解决方案专家,提供GPU硬件解决方案;CUDA解决方案专家; 面向制造业,影视动漫娱乐硬件解决方案;专业计算金融GPU解决方案;IVA GPU软硬件解决方案。

近几年，包含GPU通用计算在内的异构计算体系得到快速发展。

本文通过使用P40 GPU搭建一整套深度学习开发环境打开玩转GPU的大门，并通过部署DCGM来集中化管理GPU集群。

이 문서는 P40으로된 환경을 활용하고 있다.

환경 구성
DCGM의 환경 구성 요구사항 소개

Deep Learning 개발 환경 구축 및 운영을 위한 자원 소개
주로 사용하거나 자주 사용할 만한 명령어에 대한 정리

# NVIDIA Driver 설치

# CUDA 설치 및 Sample을 이용한 자원 확인

cd /usr/local/cuda/samples/1_Utilities/deviceQuery
make
./deviceQuery
备注: 如果打算在windows下面使用桌面gpu进行开发，安装完成后可以到C:Program FilesNVIDIA GPU Computing ToolkitCUDAv9.0extrasdemo_suite

下面执行命令deviceQuery.exe查看详细信息

# Persistance mode 설정

GPU를 활용한 개발환경을 구축하기 전 알아두면 좋은 것이 있는데, nvidia kernel mode driver입니다. 일반적으로 Driver가 설치되면 바로 장치를 이용할 수 있는 것으로 생각할 수 있는데, nvidia driver는 바로 사용할 수 있는 것이 아니라 사용하려는 GPU를 Load하고 초기화 하는 과정을 거치게 됩니다.

https://docs.nvidia.com/deploy/driver-persistence/index.html
Application start latency
Applications that trigger GPU initilization may incur a short (order of 1-3 second) startup cost per GPU due to ECC scrubbing behavior. If the GPU is already initialized this scrubbing does not take place.

Preservation of driver state
If the driver deinitializes a GPU some non-persistent state associated with that GPU will be lost and revert back to defaults the next time the GPU is initialized. See Data Persistence. To avoid this the GPU should be kept initialized.

중요한 점은 장치가 계속 사용이 되고 있으면 이 과정이 필요가 없지만,
만약 간헐적으로 사용한다면 위 같은 지연시간을 갖게 되므로 해결을 해야 합니다.

윈도우는 화면을 띄우기 때문에 계속해서 GPU를 사용하므로 사실 신경쓸 필요는 없는데,
Linux 서버를 설치한 경우 보통 X를 설치하지 않기 떄문에 GPU가 대기상태로 들어가지 않도록 설정해야 합니다.

이를 Driver Persistance라고 부르는데, 설정하는 방법이 두가지가 있습니다.

## Persistence Daemon
시스템이 설치 되
이 파일을 받아서
https://github.com/NVIDIA/nvidia-persistenced/blob/master/init/systemd/nvidia-persistenced.service.template


服务启动文件位置 /usr/lib/systemd/system/nvidia-persistenced.service，这个文件是由xorg-x11-drv-nvidia软件包提供的，如果找不到这个文件，请检查该软件包是否安装成功

systemctl start nvidia-persistenced.service
如果希望系统启动后自动进入持久化状态，可以执行：

systemctl enable nvidia-persistenced.service

systemctl status nvidia-persistenced.service

## Persistence Mode (Legacy)

{% highlight bash %}
$ nvidia-smi -pm 1
{% endhighlight %}

만약 부팅할때 자동으로 실행이 되게 하고 싶다면,
/etc/rc.d/rc.local (RedHat)
/etc/rc.local (debian)
에 위 명령을 추가하면 된다.

# DCGM 설치

1. 소개

NVIDIA Data Center GPU Manager(DCGM)은 시스템에 설치된 GPU들을 모니터링하고 관리하는 기능을 제공한다. Kepler 이후의 모든 Tesla GPU에선 모든 기능을 지원하고, 이외의 GPU에서는 Maxwell 이후 버전에서는 일부 기능만이 지원된다. DCGM에서 제공하는 기능은 다음과 같다.

* Active Health Monitoring
    * Runtime Health Checks
    * Prologue Checks
    * Epilogue Checks

* Diagnostics & System Validation
    * System Validation Tests
    * Deep HW Diagnostics

* Policy & Group Config Management
    * Pre-configured policies
    * Job Level accounting 
    * Stateful configuration

* Power & Clock Mgmt
    * Dynamic Power Capping
    * Synchronous Clock Boost

DCGM을 이용한 환경 구성은 Node를 하나만 사용해서 Standalone으로 구성할 수도 있고, Master/Agent 구조를 사용해서 복수의 GPU Compute node들을 관리하는 방식으로도 활용이 가능하다.

2. GPU를 Monitoring하는 방법

현재 GPU를 모니터링할 수 있는 방법으로는 아래와 같이 세가지 방법이 있다.

<table>
    <tr>
        <td>NVML</td>
        <td>
            * Stateless queries.Can only query current data 

            * Low overhead while running,high overhead to develop

            * Management app run on same box as GPUs

            * Low-level control of GPUs
        </td>
    </tr>
    <tr>
        <td>DCGM</td>
        <td>
            Can query a few hours of metrics

            Provides health checks and diagnostics

            Can batch queries/operations to groups of GPUs

            Can be remote or local
        </td>
    </tr>
    <tr>
        <td>3rd Party tools</td>
        <td>
            Provide database，graphs，and a nice UI

            Need management nodes

            Development already done.You just have to configure the tools
        </td>
    </tr>
</table>

GPU Compute node가 몇개 없다면 NVML로도 충분히 관리를 할 수 있겠지만, 어느정도 규모가 있는 센터가 되면 DCGM과 같은 도구를 사용한다면 Monitoring을 하는데 생산성이 향상될 수 있다. 또한 DCGM을 지원하는 3rd party software들이 다양하기 때문에 이를 이용하면 예쁜 Dashboard도 연동해서 그럴듯한 GPU monitoring 환경을 구축할 수 있다.

3. DCGM 설치

우선 Kepler 이후의 Tesla 계열의 GPU가 설치된 System을 준비한다.

Linux에서 NVIDIA Driver를 설치할 수 있는 환경이 준비되면 된다. Tesla가 아닌 GPU에서 어떤 기능들이 지원되지 않는지는 나도 잘 모르겠다. 우선 Driver는 384+ 이후의 버전을 설치할 것을 권장한다. GPU 장치 지원에 대한 SW이기 때문에 사실 최신일수록 좋다.

4. Download

[NVIDIA DCGM](https://developer.nvidia.com/data-center-gpu-manager-dcgm) 사이트로 가면 필요한 Package file과 문서를 다운받을 수 있다. 다만 NVIDIA Developer에 가입이 되어 있어야 한다.
Redhat 계열의 Linux와 Debian 계열의 Linux를 위한 배포판 패키지가 준비되어 있으므로 적절한 것을 받으면 된다.

5. 설치 확인

{% highlight bash %}
dcgmi --version
{% endhighlight %}

dcgmi  version: 1.2.3

{% highlight bash %}
$ nvvs --version
{% endhighlight %}

nvvs  version: 384.6

6. DCGM Monitoring 구성하기

앞서 다룬 ```dcgmi```와는 별개로 DCGM에서는 nv-hostengine이라는 system daemon을 통해서 GPU를 monitoring하게 할 수 있다.

{% highlight bash %}
$ sudo nv-hostengine
Started host engine version 1.4.2 using port number: 5555
{% endhighlight %}

{% highlight bash %}
$ dcgmi discovery -l
4 GPUs found.
+--------+-------------------------------------------------------------------+
| GPU ID | Device Information                                                |
+========+===================================================================+
| 0      |  Name: Tesla V100-DGXS-16GB                                       |
|        |  PCI Bus ID: 00000000:07:00.0                                     |
|        |  Device UUID: GPU-f0a4dd93-d478-9fa2-f1cd-ebe866a65079            |
+--------+-------------------------------------------------------------------+
| 1      |  Name: Tesla V100-DGXS-16GB                                       |
|        |  PCI Bus ID: 00000000:08:00.0                                     |
|        |  Device UUID: GPU-0cc2e775-e35b-9b63-dd4b-a238cac255f8            |
+--------+-------------------------------------------------------------------+
| 2      |  Name: Tesla V100-DGXS-16GB                                       |
|        |  PCI Bus ID: 00000000:0E:00.0                                     |
|        |  Device UUID: GPU-34e6b398-a7b4-b959-b49f-b23284f243f1            |
+--------+-------------------------------------------------------------------+
| 3      |  Name: Tesla V100-DGXS-16GB                                       |
|        |  PCI Bus ID: 00000000:0F:00.0                                     |
|        |  Device UUID: GPU-7c9ef90f-2a8b-1d10-2a9e-57ea68c73799            |
+--------+-------------------------------------------------------------------+
{% endhighlight %}

{% highlight bash %}
$ sudo nv-hostengine -t
Host engine successfully terminated.
{% endhighlight %}

만약 master node에서 특정 IP의 GPU 정보를 수집하게 하고자 한다면 다음과 같이 대상 Compute Node를 지정할 수 있다.

{% highlight bash %}
# sudo nv-hostengine -b 192.168.4.25
{% endhighlight %}

CLI를 이용해서 다른 장치를 보는 것도 가능하다.
{% highlight bash %}
$ dcgmi discovery -l --host 10.1.1.1
{% endhighlight %}

7.涉及组件

| 组件名称 | 说明 |
| —- | —- | |

DCGM

sharedlibrary

| 用户层共享库，libdcgm.so，是DCGM的核心组件。这个库实现了主要的底层函数，并作为一组基于C的APIs公开 |
|

NVIDIA

HostEngine

| NVIDIA 主机引擎，nv-hostengine，是对DCGM共享库的一个简单封装。主要作用是将DCGM库实例化为一个持续独立运行的进程，并配置相应的监控和管理功能。备注：DCGM可以使用root用户或者非root用户执行，但是大部分操作，比如配置管理GPU是不能使用非root用户执行的 |
|

DCGM

CLITool

| DCGM的命令行界面dcgmi，以简单的交互形式执行大部分的DCGM操作。适合不想额外编写程序操作接口控制DCGM或者收集相关数据的管理员或者用户。本程序不适合用于脚本处理 |
|

Python

Bindings

| 安装目录为/usr/src/dcgm/bindings |
|

Software

Development

Kit

| 包括如何使用DCGM的功能以及API的示例文档和头文件。SDK包括C和Python两类APIs，包括了独立和嵌入模式下两种不同环境的使用示例。安装位置为/usr/src/dcgm/sdk_samples
|

8.关键特性

备注：操作如下命令前必须先启动监听引擎

Groups

几乎所有的DCGM操作都是以group为单位进行的。用户可以创建、销毁以及修改本地节点上的GPU group，后续将以此为单位进行其他操作。Groups的概念旨在将一些GPU作为单个抽象资源进行管理，对于只有一个GPU的机器来说，完全可以忽略group概念，可以当作只有一个GPU的group进行操作。为了方便起见，DCGM会创建一个默认的group来包含集群内所有的GPU。不同的group的GPU成员可以重叠，通过创建不同的GPU group可以方便的对job级别的工作进行处理，比如查看job状态或者健康检查等管理group非常简单，使用dcgmi group子命令即可，下面示例了如何创建、列出、删除group

# dcgmi group -c GPU_Group
Successfully created group “GPU_Group” with a group ID of 1

  # dcgmi group -l

  1 group found.
+----------------------------------------------------------------------------+
| GROUPS                                                                     |
+============+===============================================================+
| Group ID   | 1                                                             |
| Group Name | GPU_Group                                                     |
| GPU ID(s)  | None                                                          |
+------------+---------------------------------------------------------------+




  # dcgmi group -d 1
Successfully removed group 1如果想要将GPU添加到group中，需要识别它们，首先列出已安装的GPU设备

  # dcgmi discovery -l4 GPUs found.
+--------+-------------------------------------------------------------------+
| GPU ID | Device Information                                                |
+========+===================================================================+
| 0      |  Name: Tesla P40                                                  |
|        |  PCI Bus ID: 00000000:02:00.0                                     |
|        |  Device UUID: GPU-8bed4da1-ac8c-5613-d74a-e71dec80c048            |
+--------+-------------------------------------------------------------------+
| 1      |  Name: Tesla P40                                                  |
|        |  PCI Bus ID: 00000000:03:00.0                                     |
|        |  Device UUID: GPU-5e7f452b-d6bb-c61e-bcb0-678a19278c0a            |
+--------+-------------------------------------------------------------------+
| 2      |  Name: Tesla P40                                                  |
|        |  PCI Bus ID: 00000000:83:00.0                                     |
|        |  Device UUID: GPU-4ee978a0-ade4-3b67-274e-b4c8083132fd            |
+--------+-------------------------------------------------------------------+
| 3      |  Name: Tesla P40                                                  |
|        |  PCI Bus ID: 00000000:84:00.0                                     |
|        |  Device UUID: GPU-8055c4f0-9523-0af1-c88a-1be7847c1434            |
+--------+-------------------------------------------------------------------+
我们当前有4块GPU卡，我们想把id为0和1的两块gpu卡添加到我们之前创建的group中：

  # dcgmi group -g 1 -a 0,1
  Add to group operation successful.
查看指定的group信息：

  # dcgmi group -g 1 -i 
+----------------------------------------------------------------------------+
| GROUPS                                                                     |
+============+===============================================================+
| Group ID   | 1                                                             |
| Group Name | GPU_Group                                                     |
| GPU ID(s)  | 0, 1                                                          |
+------------+---------------------------------------------------------------+
Configuration

管理GPU，尤其是在多节点的环境中一个重要方面就是确保跨工作负载和跨设备的配置要保持一致。

这里说的配置指的是NVIDIA公开的用于调整GPU行为的一组管理参数。
DCGM工具让客户更容易定义所需要的配置，并且确保这些配置不随着时间推移而发生改变不同的GPU参数具有不同级别的持久化属性。主要分为两大类：

1) Device InfoROM lifetime
      * 存在于电路板上面的非易失性存储器，可以保持一定的配置
      * 即便刷新固件，配置也会被保存
2) GPU initialization lifetime
      * 驱动级别的数据存储，保存一些容易变化的GPU运行数据
      * 一直保存直到GPU被内核模式驱动解除初始化状态为止
DCGM维护的主要是第二类，这类配置通常是经常变化的，每次GPU进入空闲状态或者被reset的时候都可能会发生变化。

▼

通过使用DCGM，客户端可以确保在期望的时间内保持配置不变。

▼

在大部分情况下，客户端应该为系统中所有的GPU在初始化的时候定义一个统一的配置，或者基于每一个任务定义单独的group。

一旦定义了相关配置，DCGM会在必要时强制执行该配置，比如驱动程序重启，GPU重置或者作业启动

DCGM目前支持如下配置

| Settings | Description | Defaults |
| —- | —- | —- | |

Sync

Boost

| Coordinate Auto Boost across GPUs in the group | None |
|

Target

Clocks

| Attempt to maintain fixed clocks at the target values | None |
|

ECC

Mode

| Enable ECC protection throughout the GPU’s memory |

Usually

On

|
|

Power

Limit

| Set the maximum allowed power consumption | Varies |
| Compute Mode | limit concurrent process access to the GPU |

Norestrictions

|

要为一个gpu group定义一组配置，使用dcgmi config子命令

参考如下示例:

查看某一个group(示例中为gropu 1)的配置

    # dcgmi config -g 1 --get 
+--------------------------+------------------------+------------------------+
| GPU_Group                |                        |                        |
| Group of 2 GPUs          | TARGET CONFIGURATION   | CURRENT CONFIGURATION  |
+==========================+========================+========================+
| Sync Boost               |  Not Configured        |  Disabled              |
| SM Application Clock     |  Not Configured        |  Not Specified         |
| Memory Application Clock |  Not Configured        |  Not Specified         |
| ECC Mode                 |  Not Configured        |  Enabled               |
| Power Limit              |  Not Configured        |  250                   |
| Compute Mode             |  Not Configured        |  Unrestricted          |
+--------------------------+------------------------+------------------------+
**** Non-homogenous settings across group. Use with –v flag to see details.
其中可以看到这个组里包含了两个GPU

 # dcgmi config -g 1 --set -c 2
一旦设置了一个配置，DCGM中有Target和Current两个概念。Target用于跟踪用户请求的配置状态，Current表示GPU当前的实际状态。DCGM会尝试从Current状态转为Target设置的状态。

Policy

DCGM允许客户端配置GPU在出现某些事件时自动执行特定操作。这对于event-action场景非常有用，比如当发生严重错误时自动恢复，对于event-notification场景也非常有用，比如客户端希望当出现RAS事件时收到警告信息

在以上两种情况下，客户端必须定义某些条件以便触发后续动作，这些条件是从一组预定义的可用度量值指定。

一般来说，事件分为严重故障，非严重故障或者是性能警告。包含如下情况：

最简单的策略就是当发生特定事件时让DCGM通知用户，除此之外不再进行额外的动作，通常是作为编程接口中的回调机制。

每当触发条件时，DCGM都会调用这些回调接口。一旦收到特定条件的回调，该通知注册事件就宣告终止，如果客户想要重复的通知某一个事件，应该在处理每一个回调之后重新执行注册。

以下命令用于配置一条通知策略，当检测到PCIe致命/非致命错误时进行通知：

# dcgmi policy -g 2 --set 0,0 -p 
Policy successfully set.




# dcgmi policy -g 2 --getPolicy information
+---------------------------+------------------------------------------------+
| GPU_Group                 | Policy Information                             |
+===========================+================================================+
| Violation conditions      | PCI errors and replays                         |
| Isolation mode            | Manual                                         |
| Action on violation       | None                                           |
| Validation after action   | None                                           |
| Validation failure action | None                                           |
+---------------------------+------------------------------------------------+
**** Non-homogenous settings across group. Use with –v flag to see details.
一旦设置了策略，客户端就会收到相应的通知，虽然主要是用于编程使用，我们也可以用dcgmi来接收相应的通知信息，比如

# dcgmi policy -g 2 --regListening for violations
…
A PCIe error has violated policy manager values.
...
Action策略是notification策略的超集，用于自动执行特定操作，比如自动处理特定的RAS事件。

Action policy包括三个额外的组件:

Component	Description
Isolation mode	在执行后续步骤前，决定DCGM是否需要独占GPU访问
Action	侵入式执行某些操作
Vlidation	GPU状态验证以及后期操作
每当策略运行时设置策略的客户端会收到两次信息

1) 当条件被触发，策略运行时通知回调

2) 当action运行完毕，比如验证步骤完成通知回调

配置示例：

  # dcgmi policy -g 1 --set 1,1 -e
  Policy successfully set.
  # dcgmi policy -g 1 --get
  Policy information
+---------------------------+------------------------------------------------+
| GPU_Group                 | Policy Information                             |
+===========================+================================================+
| Violation conditions      | Double-bit ECC errors                          |
| Isolation mode            | Manual                                         |
| Action on violation       | Reset GPU                                      |
| Validation after action   | NVVS (Short)                                   |
| Validation failure action | None                                           |
+---------------------------+------------------------------------------------+
**** Non-homogenous settings across group. Use with –v flag to see details.
**
****Job Stats**

以上是一个job执行流程。

DCGM提供了后台数据收集和分析的功能，可以在job执行整个生命周期内收集汇总所涉及到的GPU的数据。

要使用该功能，客户端必须先配置指定group启用stats记录功能，这会让DCGM定期查看这些GPU的所有相关指标以及设备上执行的处理活动。该操作只需要在每次初始化任务的时候执行一次即可。

# dcgmi state -g 1 --enable 
Successfully started process watches.
注意必须先启动stats记录功能，然后再启动job，否则可能信息会不准确。
一旦job执行完毕，就可以通过DCGM查询这个job相关的信息，并且可以根据需要，在该group内的GPU之间分解相关信息。

建议的方式是客户端在epilogue阶段的脚本中执行查询来作为job清理的一部分。
以下命令表示获取pid为19025这个job执行的相关统计信息

 # dcgmi stats --pid 19025 -v

  Successfully retrieved process info for PID: 19025. Process ran on 4 GPUs.
+------------------------------------------------------------------------------+
| GPU ID: 0                                                                    |
+====================================+=========================================+
|-----  Execution Stats  ------------+-----------------------------------------|
| Start Time                     *   | Tue Nov  7 09:46:42 2017                |
| End Time                       *   | Tue Nov  7 09:47:29 2017                |
| Total Execution Time (sec)     *   | 47.46                                   |
| No. of Conflicting Processes   *   | 0                                       |
+-----  Performance Stats  ----------+-----------------------------------------+
| Energy Consumed (Joules)           | 2082                                    |
| Max GPU Memory Used (bytes)    *   | 170917888                               |
| SM Clock (MHz)                     | Avg: 1220, Max: 1531, Min: 544          |
| Memory Clock (MHz)                 | Avg: 2658, Max: 3615, Min: 405          |
| SM Utilization (%)                 | Avg: 20, Max: 100, Min: 0               |
| Memory Utilization (%)             | Avg: 5, Max: 57, Min: 0                 |
| PCIe Rx Bandwidth (megabytes)      | Avg: 94, Max: 146, Min: 75              |
| PCIe Tx Bandwidth (megabytes)      | Avg: 102, Max: 158, Min: 81             |
+-----  Event Stats  ----------------+-----------------------------------------+
| Single Bit ECC Errors              | 0                                       |
| Double Bit ECC Errors              | 0                                       |
| PCIe Replay Warnings               | 0                                       |
| Critical XID Errors                | 0                                       |
+-----  Slowdown Stats  -------------+-----------------------------------------+
| Due to - Power (%)                 | 0                                       |
|        - Thermal (%)               | 0                                       |
|        - Reliability (%)           | 0                                       |
|        - Board Limit (%)           | 0                                       |
|        - Low Utilization (%)       | 0                                       |
|        - Sync Boost (%)            | 0                                       |
+-----  Process Utilization  --------+-----------------------------------------+
| PID                                | 19025                                   |
|     Avg SM Utilization (%)         | 16                                      |
|     Avg Memory Utilization (%)     | 4                                       |
+-----  Overall Health  -------------+-----------------------------------------+
| Overall Health                     | Healthy                                 |
+------------------------------------+-----------------------------------------+
对于某些框架来说，处理过程和pid和job不是直接关联的，在job执行过程中可能会产生很多的子进程。为了获取这种job的统计信息，在job启动和停止的时候必须先通知DCGM。

客户端需要在job的prologue阶段将用户定义的job id和使用的GPU group告知DCGM，并在job的epilogue阶段再次通知job id。
用户可以依据job id查询统计信息并获得这一阶段所有的聚合统计信息。

发送启动通知

  # dcgmi stats -g 1 -s
  Successfully started recording stats for
发送停止通知

  # dcgmi stats -x
  Successfully stopped recording stats for
要查看统计信息使用如下命令

# dcgmi stats -j
Health & Diagnostics

DCGm提供了多种机制来获取GPU的健康状态，不同的机制应用于不同的场景。通过使用这些接口，客户可以轻松的以非侵入的方式(不影响正在运行的任务)了解GPU的健康状态，也可以采用主动方式(当GPU没有任务可以运行一些深度诊断测试)

| 获取机制 | 描述 |
| —- | —- | |

Background

health checks

| 非侵入式健康检查，可以在跑job的时候执行，不会影响job和性能 |
|

Prologue

health checks

| 侵入式健康检查，耗时数秒钟，旨在验证提交job前GPU已经准备好了执行任务 |
|

Epilogue

health checks

| 侵入式健康检查，耗时数分钟，主要是用于job运行失败或者怀疑GPU健康状态有问题时执行 |
|

Full system

validation

| 侵入式健康检查，完整的系统检查，耗时数十分钟，可以用于诊断硬件问题或者其他严重问题 |

注意以上这几种都是在线诊断，除了GPU之外有很多的其他因素可能会影响到测试结果。要完成完整的硬件诊断以及走RMA流程都需要在离线状态下使用NVIDIA提供的专用工具进行检测

设计目的是用于识别关键问题，而不会影响应用程序的行为和性能。通过这些检查可以发现各种严重的问题，比如GPU无反应，固件损坏或者散热异常等当发生这些问题时，DCGM会报告警告或者错误，建议遵循如下处理规则
1) Warning，检测到的问题不会影响当前的job工作，但需要引起注意，并尽可能进行修复。
2) Error，检测到关键问题，目前的job可能会受到影响甚至中断，这种情况通常会引发致命的RAS事件，需要终止当前执行的工作，并执行GPU健康检查。

启用方式

 # dcgmi health -g 1 -s mpi
 Health monitor systems set successfully.
查看当前健康状态

 # dcgmi helath -g 1 -c 
Health Monitor Report
+------------------+---------------------------------------------------------+
| Overall Health:   Healthy                                                  |
+==================+=========================================================+
假如有问题，示例如下
Health Monitor Report
+----------------------------------------------------------------------------+
| Group 1 | Overall Health: Warning                                          |
+==================+=========================================================+
| GPU ID: 0 | Warning                                                        |
| | PCIe system: Warning - Detected more than 8 PCIe                         |
| | replays per minute for GPU 0: 13                                         |
+------------------+---------------------------------------------------------+
| GPU ID: 1 | Warning                                                        |
| | InfoROM system: Warning - A corrupt InfoROM has been                     |
| | detected in GPU 1.                                                       |
+------------------+---------------------------------------------------------+
主动健康检查是侵入式的检查方式，需要独占目标GPU设备。通过模拟运行一个真实的任务然后分析结果，DCGM可以确认各种各样的问题，包括通过dcgmi diag子命令，可以执行不同时间长度的检查工作。

# dcgmi diag -g 1 -r 1
 Successfully ran diagnostic for group.
+---------------------------+------------------------------------------------+
| Diagnostic                | Result                                         |
+===========================+================================================+
|-----  Deployment  --------+------------------------------------------------|
| Blacklist                 | Pass                                           |
| NVML Library              | Pass                                           |
| CUDA Main Library         | Pass                                           |
| CUDA Toolkit Libraries    | Skip                                           |
| Permissions and OS Blocks | Pass                                           |
| Persistence Mode          | Pass                                           |
| Environment Variables     | Pass                                           |
| Page Retirement           | Pass                                           |
| Graphics Processes        | Pass                                           |
+---------------------------+------------------------------------------------+
-r指定检查类型，1表示快速检查，2表示中等时间检查，3表示完整检查

诊断测试也可以作为action policy验证阶段的一部分，DCGM会将主动健康检查的日志保存到系统中。

有两种类型的日志：1) 硬件诊断会产生一个加密的二进制文件，这个文件只能由NVIDIA官方查看。2) 系统验证和压力测试检查通过JSON文本提供了额外的带有时间序列数据，可以用多种程序查看。

Topology

DCGM提供了多种机制来帮助用户了解GPU拓扑，包括详细的设备级别以及简略的group两个级别。用于提供连接到系统中的其他GPU信息以及关于NUMA/亲和力相关信息。

查看设备级别拓扑视图

# dcgmi topo --gpuid 0 
+-------------------+--------------------------------------------------------+
| GPU ID: 0         | Topology Information                                   |
+===================+========================================================+
| CPU Core Affinity | 0 - 13, 28 - 31                                        |
+-------------------+--------------------------------------------------------+
| To GPU 1          | Connected via a PCIe host bridge                       |
| To GPU 2          | Connected via a CPU-level link                         |
| To GPU 3          | Connected via a CPU-level link                         |
+-------------------+--------------------------------------------------------+
查看group级别拓扑视图

# dcgmi topo -g 1 
+-------------------+--------------------------------------------------------+
| mygpu             | Topology Information                                   |
+===================+========================================================+
| CPU Core Affinity | 0 - 13, 28 - 31                                        |
+-------------------+--------------------------------------------------------+
| NUMA Optimal      | True                                                   |
+-------------------+--------------------------------------------------------+
| Worst Path        | Connected via a PCIe host bridge                       |
+-------------------+--------------------------------------------------------+
DCGM提供了检查系统中各个链路nvlink错误计数器的方法，方便用户捕捉异常情况，观察nvlinkhi之间通讯的健康状况主要包括四种类型的错误：

Type	description
CRC FLIT Error
Data link receive flow control digit CRC

error

|
| CRC Data Error | Data link receive data CRC error |
| Replay Error | Transmit replay error |
| Recovery Error |

Transmit recovery

error

|

检查gpuid为0的nvlink错误计数器

# dcgmi nvlink -g 0
以上就是dcgm提供的一些常用的功能，更加详细的用法可以参考命令帮助。

总结

GPU强大的并行运算能力缓解了深度学习算法的训练瓶颈，从而释放了人工智能的全新潜力，也让NVIDIA顺利成为人工智能平台方案供应商。希望以上文档能够为各位小伙伴在配置NVIDIA GPU环境时提供帮助。由于本人水平有限，如有错误疏漏之处，敬请留言，我会及时修正。

参考链接

官方产品资料，包括性能数据规格等

http://www.nvidia.cn/object/tesla_product_literature_cn.html

官方软件开发工具，样本代码等

http://www.nvidia.cn/object/tesla_software_cn.html

官方管理软件，如监控/集群管理等

http://www.nvidia.cn/object/software-for-tesla-products-cn.html

Nvidia docker样例仓库

https://github.com/NVIDIA/nvidia-docker

CUDA Toolkit文档

http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html

CUDA下载地址

http://www.nvidia.com/getcuda

GPU驱动下载

http://www.nvidia.com/Download/index.aspx

关于persistence

http://docs.nvidia.com/deploy/driver-persistence/index.html

DCGM官方介绍页

https://developer.nvidia.com/data-center-gpu-manager-dcgm

不同型号显卡计算能力查询

http://developer.nvidia.com/cuda-gpus

cuDNN下载

https://developer.nvidia.com/rdp/cudnn-download

图文来源：小米运维

作者：SRE

酷达科技授权转发

推荐往期精彩资讯

******深度学习中GPU和显存分析******

****为什么大家都不戳破深度学习的本质？
****

**NVIDIA先声夺人CES：全球最强芯DRIVE Xavier武装自动驾驶
**

**News|CES 2018电子展前瞻！自动驾驶和智能家居将是重头戏
**

**知识|10 种机器学习算法的要点（附 Python代码）
**

**过分了啊，连个微信跳一跳你们都要玩套路
**

**2017 十大最受欢的迎机器学习 Python 库**

NVIDIA GPU—Tesla和GeForce的比较

知识|5个酷毙的Python工具

【圣诞快乐】给我一个圣诞帽！@微信官方@程序员@Python……

Python为什么突然火了起来？

如何利用深度学习硬件的“闲置时间”来挖矿

GPU最终会被ASIC取代吗？

普通程序员应不应该转型AI，这三个成功案例告诉你！

**知识|深度学习训练和推理有何不同？**