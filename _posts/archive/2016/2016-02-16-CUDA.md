---
layout: post
title:  "CUDA Programming 기초"
date:   2016-02-26 4:18:00
description: CUDA 프로그래밍의 간단한 소개
published: false
categories: CUDA
tags:
 - CUDA Programming
---

### CUDA 프로그래밍의 시작
CUDA 프로그래밍의 개념을 접할 수 있도록 간단하게 CUDA 프로그램을 만들어보도록 하겠습니다.
수행할 내용은 다음과 같습니다.

{% highlight c++ %}
#define BUF_SIZE 100

// 메모리 초기화
pa = new int[BUF_SIZE];
pb = new int[BUF_SIZE];
pc = new int[BUF_SIZE];

// 데이터 초기화
for (i = 0; i < BUF_SIZE; i++) {
  a[i] = i;
  b[i] = i * BUF_SIZE;
}

// 연산 (커널 실행))
for (i = 0; i < BUF_SIZE; i++) {
  c[i] = a[i] + b[i];
}

// 결과 확인
for (i = 0; i < BUF_SIZE; i++) {
  cout << i << ": " <<c[i]
}

delete [] pa;
delete [] pb;
delete [] pc;
{% endhighlight %}

C를 해보신 분이라면 쓱 보기만 하셔도 어떤 결과가 나올지 예상하실 수 있을 겁니다. Loop을 합칠 수도 있고 굳이 이렇게 안해도 된다고 생각하실 수 있습니다. 제가 이렇게 코드를 짜 놓은 이유는 CUDA에서 어떻게 위 코드가 매치가 되는지 보여드리려고 하는 것이니까 다음 코드를 보고 설명을 드리도록 하겠습니다.

{% highlight c++ %}
define BUF_SIZE 100

__global__ void VedAdd(int *a, int *b, int *c) {
  int idx = blockDim.x * blockIdx.x + threadIdx.x;
  if (idx < BUF_SIZE)
    c[idx] = a[idx] + b[idx];
}

// 메모리 초기화
pa = new int[BUF_SIZE];
pb = new int[BUF_SIZE];
pc = new int[BUF_SIZE];

// CUDA Device Memory 초기화
int *pda, *pdb, *pdc;
cudaMalloc(&pda, BUF_SIZE * sizeof(int))
cudaMalloc(&pdb, BUF_SIZE * sizeof(int))
cudaMalloc(&pdc, BUF_SIZE * sizeof(int))

// 데이터 초기화
for (i = 0; i < BUF_SIZE; i++) {
  a[i] = i;
  b[i] = i * BUF_SIZE;
}

// 데이터 복사
cudaMemcpy(pda, pa, BUF_SIZE * sizeof(int), cudaMemcpyHostToDevice);
cudaMemcpy(pdb, pb, BUF_SIZE * sizeof(int), cudaMemcpyHostToDevice);

// 커널 실행
int threadsPerBlock = 256;
int blocksPerGrid = (BUF_SIZE + threadsPerBlock - 1) / threadsPerBlock;
VecAdd<<<blocksPerGrid, threadsPerBlock>>>(pda, pdb, pdc);

// 데이터 복사
cudaMemcpy(pc, pdc, BUF_SIZE * sizeof(int), cudaMemcpyDeviceToHost);

// 결과 확인
for (i = 0; i < BUF_SIZE; i++) {
  cout << i << ": " <<c[i]
}

// 메모리 해제
cudaFree(pda);
cudaFree(pdb);
cudaFree(pdc);

delete [] pa;
delete [] pb;
delete [] pc;
{% endhighlight %}

원래 하려는 동작에 비해서 코드가 많이 늘어나게 되었는데, 그것은 CUDA가 동작하기 위해서 필요한 자원들을 할당하는 절차들이 코드로 들어가서 그렇습니다.

그 차이점은 다음과 같습니다.
1. CUDA 메모리를 할당하고 해제한다.
1. CUDA 메모리에 데이터를 복사해야 한다.
1. CPU에서 For 문제 의해 했던 것을 cuda thread 수로 계산을 해야한다.
1. CUDA에서는 __global__이라는 키워드가 붙은 함수로 CUDA 연산을 구동시킨다.

자세한 문법은 다음에 설명하기로 하겠습니다. 다만 GPU연산을 한다는 것이 어떤 절차들이 들어가게 되는지만 이애하시면 됩니다.

### 동작 원리
[![]({{site.info.baseurl}}/images//RTC08-ERTW-Nvidia-FigX_original_large.jpg)](https://en.wikipedia.org/wiki/CUDA#/media/File:CUDA_processing_flow_(En).PNG)

설명이 잘 되어 있는 페이지가 있어서 위 그림에 링크를 했습니다.
앞서 설명했었지만 GPU는 CPU와는 별도의 메모리 구조를 갖고 있고, 따라서 별개의 메모리를 사용합니다. 주소 공간을 분할해서 쓰는 것이 아니라 아예 다른 메모리입니다. 그래픽 카드를 살때 보면 옆에 1GB, 2GB.. 등으로 메모리 용량을 표시하는데 그 메모리를 사용하는 것입니다.

**CUDA 메모리**
위 코드 블럭에서 보여드린 cudaMalloc, cudaMemcpy 등의 함수는 우리가 만든 프로그램으로 하여금 GPU에서 사용할 메모리를 확보하고, 연산에 사용할 데이터를 준비하거나 연산 결과를 가져오는 역할을 하게 됩니다. 위 그림에서 1번과 4번에 해당하는 역할을 cudaMemcpy가 수행하는 것입니다. 복사하는 방향은 (1)cudaMemcpyHostToDevice, (4)cudaMemcpyDeviceToHost로 구별할 수 있습니다.

**CUDA 실행**
CPU와 CUDA의 동작의 차이점 중에서 4번이 'CUDA 연산을 구동시킨다'였습니다. 그게 위 그림에서 3번에 해당하는 내용이고, GPU로 하여금 지정된 Kernel함수를 호출하게 하는 역할입니다. 그럼 위 코드에서 ```__global__``` 키워드로 지정되어 있는 VecAdd 함수를 GPU에서 실행하게 됩니다.

그리고 실행하는데 얼마나 어떻게 실행해야할지, 데이터의 크기는 얼마나 되는지 등을 CUDA Scheduler에게 알려주는데요, 이게 위 그림에서 2번에 해당하는 내용이고, 위 코드에서는 ....라인에 해당하는 내용입니다. 이는 CUDA가 여러개의 thread를 구동하는 하는데 필요한 정보를 전달하는 역할을 합니다.
